# Link Profiler Configuration

# Logging settings
logging:
  level: DEBUG # DEBUG, INFO, WARNING, ERROR, CRITICAL
  config: # Optional: Full logging configuration dictionary (Python dictConfig format)
    version: 1
    disable_existing_loggers: False
    formatters:
      standard:
        format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: standard
        level: DEBUG # Set handler level to DEBUG as well
    loggers:
      Link_Profiler:
        handlers: [console]
        level: DEBUG # Set logger level to DEBUG
        propagate: False
      uvicorn:
        handlers: [console]
        level: INFO
        propagate: False
      uvicorn.access:
        handlers: [console]
        level: INFO
        propagate: False
      sqlalchemy.engine:
        handlers: [console]
        level: WARNING # Set to INFO or DEBUG for more SQLAlchemy logging
        propagate: False
    root:
      handlers: [console]
      level: DEBUG

# API settings
api:
  host: "0.0.0.0"
  port: 8000
  # Publicly accessible URL for the main API (coordinator).
  # This MUST be set to the HTTPS URL if your dashboard is served over HTTPS,
  # otherwise, you will encounter "Mixed Content" errors in the browser.
  # Example: "https://monitor.yspanel.com:8000"
  external_url: "https://api.yspanel.com" 
  # Internal URL for the main API, used for backend-to-backend communication (e.g., dashboard to main API).
  # This should be the address accessible within your internal network (e.g., Docker service service_name, localhost).
  internal_url: "http://localhost:8000" # Default for local development, change for Docker/Kubernetes

# Database settings (PostgreSQL)
database:
  url: "${LP_DATABASE_URL}"

# Redis settings (for queues and caching)
redis:
  url: "${LP_REDIS_URL}"
  cache_ttl: 3600 # Default cache TTL in seconds (1 hour)

# Queue system settings
queue:
  job_queue_name: "crawl_jobs"
  result_queue_name: "crawl_results"
  dead_letter_queue_name: "dead_letter_queue"
  scheduler_interval: 5 # How often (in seconds) the coordinator checks for scheduled jobs
  heartbeat_queue_sorted_name: "crawler_heartbeats_sorted" # New: Name of the sorted set for crawler heartbeats

# Crawler settings
crawler:
  max_depth: 3
  max_pages: 1000
  # delay_seconds: 1.0 # Removed, now handled by adaptive_rate_limiter
  user_agent: "LinkProfilerBot/1.0 (+http://linkprofiler.com/bot.html)" # Default user agent
  respect_robots_txt: true # Default to true, can be overridden by job config
  follow_redirects: true
  extract_images: true
  extract_pdfs: false
  max_file_size_mb: 10
  max_retries: 3
  retry_delay_seconds: 5.0
  
# Anti-detection & Quality Assurance settings
anti_detection:
  user_agent_rotation: true # Enable user agent rotation
  consistent_ua_per_domain: true # Use same UA per domain vs random each request
  request_header_randomization: true # Enable randomizing other request headers
  human_like_delays: true # Enable human-like random delays
  random_delay_range: [0.5, 2.0] # Range for random delays in seconds
  stealth_mode: true # Applies to Playwright
  browser_fingerprint_randomization: false # Applies to Playwright
  # ml_rate_optimization: false # Moved to rate_limiting section
  captcha_solving_enabled: false # Placeholder for CAPTCHA solving integration

quality_assurance:
  spam_filtering: true
  data_quality_scoring: true

# Proxy management settings
proxy:
  use_proxies: false
  proxy_list: [] # Example: [{'url': 'http://user:pass@ip:port', 'region': 'us-east'}]
  proxy_retry_delay_seconds: 300 # How long to blacklist a bad proxy

# Browser-based crawling settings (for SPA content, Lighthouse, SERP crawling)
browser_crawler:
  enabled: true
  browser_type: "chromium" # chromium, firefox, webkit
  headless: true # Run browser in headless mode

# External API Integrations
api_cache:
  enabled: true
  ttl: 3600 # Default cache TTL for API responses in seconds

domain_api:
  abstract_api:
    enabled: false
    api_key: "${LP_DOMAIN_ABSTRACT_API_KEY}" # Your AbstractAPI key for domain validation/WHOIS
    base_url: "https://domain-validation.abstractapi.com/v1/" # Base URL for AbstractAPI Domain Validation
    whois_base_url: "https://whois.abstractapi.com/v1/" # Base URL for AbstractAPI WHOIS
  real_api:
    enabled: false
    api_key: "${LP_DOMAIN_REAL_API_KEY}" # Your API key for a real domain provider (e.g., WhoisXMLAPI, DomainTools)
    base_url: "https://api.whoisxmlapi.com/v1" # WhoisXMLAPI base URL
  whois_json_api: # New: WHOIS-JSON.com API
    enabled: false
    base_url: "https://www.whois-json.com/api/v1/whois"
    api_key: "${LP_WHOIS_JSON_API_KEY}" # WHOIS-JSON.com API key
  dns_over_https_api: # New: DNS over HTTPS (Cloudflare/Google)
    enabled: false
    cloudflare_url: "https://cloudflare-dns.com/dns-query"
    google_url: "https://dns.google/resolve"
  # Live/Cache settings for Domain Service
  domain_service:
    allow_live: true
    staleness_threshold_hours: 24 # Data considered stale after 24 hours

backlink_api:
  gsc_api:
    enabled: false # Requires manual OAuth setup and credentials.json
    credentials_file: "credentials.json" # Path to your Google API credentials.json
    token_file: "token.json" # Path to your Google API token.json
  openlinkprofiler_api:
    enabled: false # Free API with limits
    base_url: "http://www.openlinkprofiler.org/api/index.php" # OpenLinkProfiler API base URL
  real_api:
    enabled: false
    api_key: "${LP_BACKLINK_REAL_API_KEY}" # Your API key for a real backlink provider (e.g., Ahrefs, Moz, SEMrush)
    base_url: "https://api.ahrefs.com/v3" # Ahrefs API base URL
  # Live/Cache settings for Backlink Service
  backlink_service:
    allow_live: true
    staleness_threshold_hours: 24 # Data considered stale after 24 hours

serp_api:
  real_api:
    enabled: false
    api_key: "${LP_SERP_REAL_API_KEY}" # Your API key for a real SERP API (e.g., SerpApi, BrightData SERP API)
    base_url: "https://serpapi.com/search" # SerpApi base URL
  pagespeed_insights_api: # New: Google PageSpeed Insights API
    enabled: false
    api_key: "${LP_PAGESPEED_API_KEY}" # Your Google Cloud API Key
    base_url: "https://www.googleapis.com/pagespeedonline/v5/runPagespeed"
  # Live/Cache settings for SERP Service
  serp_service:
    allow_live: true
    staleness_threshold_hours: 12 # Data considered stale after 12 hours

serp_crawler:
  playwright:
    enabled: true # Use Playwright for SERP crawling
    headless: true
    browser_type: "chromium"

keyword_api:
  real_api:
    enabled: false
    api_key: "${LP_KEYWORD_REAL_API_KEY}" # Your API key for a real keyword research API (e.g., Ahrefs, SEMrush)
    base_url: "https://api.ahrefs.com/v3" # Ahrefs API base URL
  metrics_api:
    enabled: false
    api_key: "${LP_KEYWORD_METRICS_API_KEY}" # Your API key for a real keyword metrics API (e.g., Google Ads API, Ahrefs)
    base_url: "https://googleads.googleapis.com/v16" # Google Ads API base URL
  google_trends_api: # New: Google Trends (pytrends)
    enabled: false
    # pytrends is unofficial, no API key needed, but uses Google's infrastructure
    # hl and tz can be configured in client if needed
  # Live/Cache settings for Keyword Service
  keyword_service:
    allow_live: true
    staleness_threshold_hours: 24 # Data considered stale after 24 hours

# AI Service Integration (e.g., OpenRouter, OpenAI, Anthropic)
ai:
  enabled: false
  openrouter_api_key: "${LP_AI_OPENROUTER_API_KEY}" # Your OpenRouter API key
  models: # Map task types to specific models available on OpenRouter
    content_scoring: "anthropic/claude-3-haiku"
    content_classification: "anthropic/claude-3-haiku"
    content_gap_analysis: "openai/gpt-4-turbo"
    keyword_research: "google/gemini-pro"
    technical_seo_analysis: "anthropic/claude-3-5-sonnet"
    competitor_analysis: "openai/gpt-4"
    content_generation: "anthropic/claude-3-5-sonnet"
    domain_value_analysis: "mistralai/mistral-7b-instruct"
    content_nlp_analysis: "anthropic/claude-3-haiku"
    topic_clustering: "mistralai/mistral-7b-instruct"
    content_quality_assessment: "anthropic/claude-3-haiku" # Added for content quality assessment
    video_frame_analysis: "openai/gpt-4o" # Model used for analyzing individual video frames
  # Live/Cache settings for AI Service
  ai_service:
    allow_live: true
    staleness_threshold_hours: 24 # Data considered stale after 24 hours

# Technical Auditor (Lighthouse CLI)
technical_auditor:
  lighthouse_path: "lighthouse" # Path to Lighthouse CLI executable (e.g., "C:\Users\YourUser\AppData\Roaming\npm\lighthouse.cmd" on Windows)
  ssl_labs_api: # New: SSL Labs API
    enabled: false
    base_url: "https://api.ssllabs.com/api/v3/analyze"
  security_trails_api: # New: SecurityTrails API
    enabled: false
    api_key: "${LP_SECURITY_TRAILS_API_KEY}" # Your SecurityTrails API Key
    base_url: "https://api.securitytrails.com/v1"
  # Live/Cache settings for Technical Auditor
  technical_auditor_service:
    allow_live: true
    staleness_threshold_hours: 72 # Data considered stale after 72 hours (3 days)

# Social Media Integration
social_media_crawler:
  enabled: false
  platforms: ["twitter", "facebook", "linkedin", "reddit", "youtube"] # Supported platforms
  twitter_api_key: "${LP_TWITTER_API_KEY}" # Your Twitter/X API Key
  twitter_api_secret: "${LP_TWITTER_API_SECRET}" # Your Twitter/X API Secret
  twitter_bearer_token: "${LP_TWITTER_BEARER_TOKEN}" # Your Twitter/X Bearer Token (for v2 API)
  facebook_app_id: "${LP_FACEBOOK_APP_ID}" # Your Facebook App ID
  facebook_app_secret: "${LP_FACEBOOK_APP_SECRET}" # Your Facebook App Secret
  linkedin_client_id: "${LP_LINKEDIN_CLIENT_ID}" # Your LinkedIn Client ID
  linkedin_client_secret: "${LP_LINKEDIN_CLIENT_SECRET}" # Your LinkedIn Client Secret
  reddit_api: # New: Reddit API (PRAW)
    enabled: false
    client_id: "${LP_REDDIT_CLIENT_ID}" # Your Reddit Client ID
    client_secret: "${LP_REDDIT_CLIENT_SECRET}" # Your Reddit Client Secret
    user_agent: "LinkProfilerBot/1.0" # Custom user agent for Reddit API
  youtube_api: # New: YouTube Data API v3
    enabled: false
    api_key: "${LP_YOUTUBE_API_KEY}" # Your Google Cloud API Key
    base_url: "https://www.googleapis.com/youtube/v3"
  news_api: # New: NewsAPI.org
    enabled: false
    api_key: "${LP_NEWS_API_KEY}" # Your NewsAPI.org API Key
    base_url: "https://newsapi.org/v2"
  # Live/Cache settings for Social Media Service
  social_media_service:
    allow_live: true
    staleness_threshold_hours: 24 # Data considered stale after 24 hours

# Web3 Integration
web3_crawler:
  enabled: false
  ipfs_gateway_url: "https://ipfs.io/ipfs/" # Default public IPFS gateway
  blockchain_node_url: "https://mainnet.infura.io/v3/${LP_INFURA_PROJECT_ID}" # Ethereum node with your Infura Project ID
  etherscan_api_key: "${LP_ETHERSCAN_API_KEY}" # Your Etherscan API Key (for blockchain data)
  opensea_api_key: "${LP_OPENSEA_API_KEY}" # Your OpenSea API Key (for NFT data)
  # Add other Web3 API keys as needed (e.g., for Polygonscan, Covalent, etc.)
  # Live/Cache settings for Web3 Service
  web3_service:
    allow_live: true
    staleness_threshold_hours: 24 # Data considered stale after 24 hours

# Historical Data Integration
historical_data:
  wayback_machine_api: # New: Wayback Machine API
    enabled: false
    base_url: "http://web.archive.org/cdx/search/cdx"
  common_crawl_api: # New: Common Crawl API
    enabled: false
    base_url: "https://index.commoncrawl.org"
  # Live/Cache settings for Wayback Machine Client
  wayback_machine_client:
    allow_live: true
    staleness_threshold_hours: 168 # Data considered stale after 168 hours (7 days)

# Local SEO Integration
local_seo:
  nominatim_api: # New: OpenStreetMap Nominatim API
    enabled: false
    base_url: "https://nominatim.openstreetmap.org/search"
    user_agent: "LinkProfilerApp/1.0 (your_email@example.com)" # Required for Nominatim

# Notifications & Alerting
notifications:
  webhooks:
    enabled: true
    urls: [] # List of webhook URLs to send job completion notifications
  email:
    enabled: false
    smtp_server: "${LP_SMTP_SERVER}"
    smtp_port: "${LP_SMTP_PORT:-587}"
    smtp_username: "${LP_SMTP_USERNAME}"
    smtp_password: "${LP_SMTP_PASSWORD}"
    sender_email: "${LP_SENDER_EMAIL}"
  slack:
    enabled: false
    webhook_url: "${LP_SLACK_WEBHOOK_URL}"

# Monitoring settings
monitoring:
  monitor_port: 8001 # Port for the monitoring dashboard and Prometheus metrics
  performance_window: 3600 # Time window in seconds for performance calculations (e.g., jobs per hour)
  max_job_history: 50 # Max number of recent jobs to show on dashboard
  monitor_auth: # New: Credentials for the monitor to authenticate with the main API
    username: "${LP_MONITOR_USERNAME}" # IMPORTANT: Set via environment variable
    password: "${LP_MONITOR_PASSWORD}" # IMPORTANT: Use a strong password
  enabled: true # Enable/disable health monitoring
  metrics_interval: 30 # Interval in seconds for collecting metrics
  health_check_interval: 60 # Interval in seconds for running health checks
  alert_cooldown: 300 # Cooldown period for alerts in seconds

# Circuit Breaker settings
circuit_breaker:
  enabled: true # Set to true to enable distributed circuit breaker
  failure_threshold: 5           # Failures before opening
  recovery_timeout: 60           # Seconds before trying half-open
  success_threshold: 3           # Successes needed to close from half-open
  timeout_duration: 30           # Request timeout in seconds
  redis_key_prefix: "cb_state:" # Redis key prefix for circuit breaker states

# Rate Limiting settings
rate_limiting:
  ml_enhanced: true
  min_delay: 0.1
  max_delay: 30.0
  learning_rate: 0.1
  aggression_factor: 0.8

# Queue System settings
queue_system:
  redis_url: "${LP_REDIS_URL}"
  max_queue_size: 100000
  domain_max_concurrent: 2
  scheduled_jobs_queue: "scheduled_crawl_jobs" # New: Name of the queue for scheduled jobs

# Connection Optimization settings
connection_optimization:
  max_connections: 250 # Total maximum connections in the pool (Increased from 200)
  max_connections_per_host: 25 # Maximum connections per single host (Increased from 20)
  timeout_total: 30 # Total request timeout in seconds
  timeout_connect: 10 # Connection timeout in seconds
  timeout_sock_read: 30 # Socket read timeout in seconds
  retry_attempts: 3 # Number of times to retry a failed request
  retry_delay_base: 0.5 # Base delay for retries in seconds (exponential backoff)
  connection_health_check: true # Enable active health checks on connections

# Advanced Session Manager settings
advanced_session_manager:
  enabled: true
  initial_pool_size: 10
  max_pool_size_per_domain: 25
  adaptation_interval: 45

# Satellite Crawler settings
satellite:
  id: "default-satellite-1" # Unique ID for this satellite instance. Change for each instance.
  heartbeat_interval: 5 # Seconds between heartbeats
  logging: # Satellite-specific logging configuration
    level: INFO
    config:
      version: 1
      disable_existing_loggers: False
      formatters:
        standard:
          format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
      handlers:
        console:
          class: logging.StreamHandler
          formatter: standard
          filename: "satellite_crawler.log"
          maxBytes: 10485760
          backupCount: 5
          level: INFO
      loggers:
        Link_Profiler:
          handlers: [console, file]
          level: INFO
          propagate: False
        # Add other loggers specific to satellite if needed
      root:
        handlers: [console, file]
        level: INFO

# Authentication settings
auth:
  secret_key: "${LP_AUTH_SECRET_KEY}" # REQUIRED: This is your generated key
  algorithm: "HS256"
  access_token_expire_minutes: 30 # Token expiry time in minutes

# System-wide settings
system:
  # Current version of the deployed code. Update this value on every new deployment
  # to trigger automatic restarts of outdated satellite crawlers.
  current_code_version: "1.0.3" # Example: "1.0.0", "1.0.1", "2.0.0-beta"
  version_control_enabled: false # New: Control whether version comparison and auto-restart is active
