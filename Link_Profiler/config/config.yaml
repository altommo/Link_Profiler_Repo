# Link Profiler Configuration

# Logging settings
logging:
  level: DEBUG # DEBUG, INFO, WARNING, ERROR, CRITICAL
  config: # Optional: Full logging configuration dictionary (Python dictConfig format)
    version: 1
    disable_existing_loggers: False
    formatters:
      standard:
        format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: standard
        level: DEBUG # Set handler level to DEBUG as well
    loggers:
      Link_Profiler:
        handlers: [console]
        level: DEBUG # Set logger level to DEBUG
        propagate: False
      uvicorn:
        handlers: [console]
        level: INFO
        propagate: False
      uvicorn.access:
        handlers: [console]
        level: INFO
        propagate: False
      sqlalchemy.engine:
        handlers: [console]
        level: WARNING # Set to INFO or DEBUG for more SQLAlchemy logging
        propagate: False
    root:
      handlers: [console]
      level: DEBUG # Set root level to DEBUG

# API settings
api:
  host: "0.0.0.0"
  port: 8000
  # Publicly accessible URL for the main API (coordinator).
  # This MUST be set to the HTTPS URL if your dashboard is served over HTTPS,
  # otherwise, you will encounter "Mixed Content" errors in the browser.
  # Example: "https://monitor.yspanel.com:8000"
  external_url: "https://api.yspanel.com" 

# Database settings (PostgreSQL)
database:
  url: "postgresql://postgres:postgres@localhost:5432/link_profiler_db"

# Redis settings (for queues and caching)
redis:
  url: "redis://:redis_secure_pass_456@127.0.0.1:6379/0" # Redis URL for job queue and cache
  cache_ttl: 3600 # Default cache TTL in seconds (1 hour)

# Queue system settings
queue:
  job_queue_name: "crawl_jobs"
  result_queue_name: "crawl_results"
  dead_letter_queue_name: "dead_letter_queue"
  scheduler_interval: 5 # How often (in seconds) the coordinator checks for scheduled jobs

# Crawler settings
crawler:
  max_depth: 3
  max_pages: 1000
  delay_seconds: 1.0
  timeout_seconds: 30
  user_agent: "LinkProfilerBot/1.0 (+http://linkprofiler.com/bot.html)"
  respect_robots_txt: true
  follow_redirects: true
  extract_images: true
  extract_pdfs: false
  max_file_size_mb: 10
  max_retries: 3
  retry_delay_seconds: 5.0
  
# Anti-detection & Quality Assurance settings
anti_detection:
  user_agent_rotation: false
  request_header_randomization: false
  human_like_delays: false
  stealth_mode: true # Applies to Playwright
  browser_fingerprint_randomization: false # Applies to Playwright
  ml_rate_optimization: false # Placeholder for future ML-driven rate limiting
  captcha_solving_enabled: false # Placeholder for CAPTCHA solving integration

quality_assurance:
  spam_filtering: true
  data_quality_scoring: true

# Proxy management settings
proxy:
  use_proxies: false
  proxy_list: [] # Example: [{'url': 'http://user:pass@ip:port', 'region': 'us-east'}]
  proxy_retry_delay_seconds: 300 # How long to blacklist a bad proxy

# Browser-based crawling settings (for SPA content, Lighthouse, SERP crawling)
browser_crawler:
  enabled: true
  browser_type: "chromium" # chromium, firefox, webkit
  headless: true # Run browser in headless mode

# External API Integrations
api_cache:
  enabled: true
  ttl: 3600 # Default cache TTL for API responses in seconds

domain_api:
  abstract_api:
    enabled: false
    api_key: "" # Your AbstractAPI key for domain validation/WHOIS
    base_url: "https://domain-validation.abstractapi.com/v1/" # Base URL for AbstractAPI Domain Validation
    whois_base_url: "https://whois.abstractapi.com/v1/" # Base URL for AbstractAPI WHOIS
  real_api:
    enabled: false
    api_key: "" # Your API key for a real domain provider (e.g., WhoisXMLAPI, DomainTools)
    base_url: "https://api.example-domain-provider.com" # Example base URL for a real domain API
  whois_json_api: # New: WHOIS-JSON.com API
    enabled: false
    base_url: "https://www.whois-json.com/api/v1/whois"
    api_key: "" # WHOIS-JSON.com might require an API key for higher limits
  dns_over_https_api: # New: DNS over HTTPS (Cloudflare/Google)
    enabled: false
    cloudflare_url: "https://cloudflare-dns.com/dns-query"
    google_url: "https://dns.google/resolve"

backlink_api:
  gsc_api:
    enabled: false # Requires manual OAuth setup and credentials.json
    credentials_file: "credentials.json" # Path to your Google API credentials.json
    token_file: "token.json" # Path to your Google API token.json
  openlinkprofiler_api:
    enabled: false # Free API with limits
    base_url: "http://www.openlinkprofiler.org/api/index.php" # OpenLinkProfiler API base URL
  real_api:
    enabled: false
    api_key: "" # Your API key for a real backlink provider (e.g., Ahrefs, Moz, SEMrush)
    base_url: "https://api.example-backlink-provider.com" # Example base URL for a real backlink API

serp_api:
  real_api:
    enabled: false
    api_key: "" # Your API key for a real SERP API (e.g., SerpApi, BrightData SERP API)
    base_url: "https://api.example-serp-provider.com" # Example base URL for a real SERP API
  pagespeed_insights_api: # New: Google PageSpeed Insights API
    enabled: false
    api_key: "" # Your Google Cloud API Key
    base_url: "https://www.googleapis.com/pagespeedonline/v5/runPagespeed"

serp_crawler:
  playwright:
    enabled: true # Use Playwright for SERP crawling
    headless: true
    browser_type: "chromium"

keyword_api:
  real_api:
    enabled: false
    api_key: "" # Your API key for a real keyword research API (e.g., Ahrefs, SEMrush)
    base_url: "https://api.example-keyword-research.com" # Example base URL for a real keyword research API
  metrics_api:
    enabled: false
    api_key: "" # Your API key for a real keyword metrics API (e.g., Google Ads API, Ahrefs)
    base_url: "https://api.example-keyword-metrics.com" # Example base URL for a real keyword metrics API
  google_trends_api: # New: Google Trends (pytrends)
    enabled: false
    # pytrends is unofficial, no API key needed, but uses Google's infrastructure
    # hl and tz can be configured in client if needed

# AI Service Integration (e.g., OpenRouter, OpenAI, Anthropic)
ai:
  enabled: false
  openrouter_api_key: "" # Your OpenRouter API key
  models: # Map task types to specific models available on OpenRouter
    content_scoring: "mistralai/mistral-7b-instruct"
    content_classification: "mistralai/mistral-7b-instruct"
    content_gap_analysis: "mistralai/mistral-7b-instruct"
    keyword_research: "mistralai/mistral-7b-instruct"
    technical_seo_analysis: "mistralai/mistral-7b-instruct"
    competitor_analysis: "mistralai/mistral-7b-instruct"
    content_generation: "mistralai/mistral-7b-instruct"
    domain_value_analysis: "mistralai/mistral-7b-instruct"
    content_nlp_analysis: "mistralai/mistral-7b-instruct"
    topic_clustering: "mistralai/mistral-7b-instruct"

# Technical Auditor (Lighthouse CLI)
technical_auditor:
  lighthouse_path: "lighthouse" # Path to Lighthouse CLI executable (e.g., "C:\Users\YourUser\AppData\Roaming\npm\lighthouse.cmd" on Windows)
  ssl_labs_api: # New: SSL Labs API
    enabled: false
    base_url: "https://api.ssllabs.com/api/v3/analyze"
  security_trails_api: # New: SecurityTrails API
    enabled: false
    api_key: "" # Your SecurityTrails API Key
    base_url: "https://api.securitytrails.com/v1"

# Social Media Integration
social_media_crawler:
  enabled: false
  platforms: ["twitter", "facebook", "linkedin", "reddit", "youtube"] # Supported platforms
  twitter_api_key: "" # Your Twitter/X API Key
  twitter_api_secret: "" # Your Twitter/X API Secret
  twitter_bearer_token: "" # Your Twitter/X Bearer Token (for v2 API)
  facebook_app_id: "" # Your Facebook App ID
  facebook_app_secret: "" # Your Facebook App Secret
  linkedin_client_id: "" # Your LinkedIn Client ID
  linkedin_client_secret: "" # Your LinkedIn Client Secret
  reddit_api: # New: Reddit API (PRAW)
    enabled: false
    client_id: "" # Your Reddit Client ID
    client_secret: "" # Your Reddit Client ID
    user_agent: "LinkProfilerBot/1.0" # Custom user agent for Reddit API
  youtube_api: # New: YouTube Data API v3
    enabled: false
    api_key: "" # Your Google Cloud API Key
    base_url: "https://www.googleapis.com/youtube/v3"
  news_api: # New: NewsAPI.org
    enabled: false
    api_key: "" # Your NewsAPI.org API Key
    base_url: "https://newsapi.org/v2"

# Web3 Integration
web3_crawler:
  enabled: false
  ipfs_gateway_url: "https://ipfs.io/ipfs/" # Default public IPFS gateway
  blockchain_node_url: "https://mainnet.infura.io/v3/YOUR_INFURA_PROJECT_ID" # Example Ethereum node (replace with your Infura Project ID)
  etherscan_api_key: "" # Your Etherscan API Key (for blockchain data)
  opensea_api_key: "" # Your OpenSea API Key (for NFT data)
  # Add other Web3 API keys as needed (e.g., for Polygonscan, Covalent, etc.)

# Historical Data Integration
historical_data:
  wayback_machine_api: # New: Wayback Machine API
    enabled: false
    base_url: "http://web.archive.org/cdx/search/cdx"
  common_crawl_api: # New: Common Crawl API
    enabled: false
    base_url: "https://index.commoncrawl.org"

# Local SEO Integration
local_seo:
  nominatim_api: # New: OpenStreetMap Nominatim API
    enabled: false
    base_url: "https://nominatim.openstreetmap.org/search"
    user_agent: "LinkProfilerApp/1.0 (your_email@example.com)" # Required for Nominatim

# Notifications & Alerting
notifications:
  webhooks:
    enabled: true
    urls: [] # List of webhook URLs to send job completion notifications
  email:
    enabled: false
    smtp_server: ""
    smtp_port: 587
    smtp_username: ""
    smtp_password: ""
    sender_email: ""
  slack:
    enabled: false
    webhook_url: "" # Slack incoming webhook URL

# Monitoring settings
monitoring:
  monitor_port: 8001 # Port for the monitoring dashboard and Prometheus metrics
  performance_window: 3600 # Time window in seconds for performance calculations (e.g., jobs per hour)
  max_job_history: 50 # Max number of recent jobs to show on dashboard
  monitor_auth: # New: Credentials for the monitor to authenticate with the main API
    username: "monitor_user" # IMPORTANT: Create this user in your main API's database
    password: "monitor_secure_password_123" # IMPORTANT: Use a strong password

# Authentication settings
auth:
  secret_key: "xKroXcaIePQydhdhS4GMhdMfTsjhKzthaoL5OmU5MBA" # REQUIRED: This is your generated key
  algorithm: "HS256"
  access_token_expire_minutes: 30 # Token expiry time in minutes

# System-wide settings
system:
  # Current version of the deployed code. Update this value on every new deployment
  # to trigger automatic restarts of outdated satellite crawlers.
  current_code_version: "1.0.2" # Example: "1.0.0", "1.0.1", "2.0.0-beta"
